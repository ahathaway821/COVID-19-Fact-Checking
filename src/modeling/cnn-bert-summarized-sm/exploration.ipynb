{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/84/23ed6a1796480a6f1a2d38f2802901d078266bda38388954d01d3f2e821d/pip-20.1.1-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 5.6MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.3.1\n",
      "    Uninstalling pip-19.3.1:\n",
      "      Successfully uninstalled pip-19.3.1\n",
      "Successfully installed pip-20.1.1\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-49.1.0-py3-none-any.whl (789 kB)\n",
      "\u001b[K     |████████████████████████████████| 789 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 45.2.0\n",
      "    Uninstalling setuptools-45.2.0:\n",
      "      Successfully uninstalled setuptools-45.2.0\n",
      "Successfully installed setuptools-49.1.0\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2 MB 5.7 kB/s  eta 0:00:01  |▍                               | 6.8 MB 5.0 MB/s eta 0:01:42     |██▋                             | 41.8 MB 5.0 MB/s eta 0:01:35     |█████▏                          | 82.8 MB 73.5 MB/s eta 0:00:06     |██████▌                         | 104.1 MB 49.3 MB/s eta 0:00:09     |████████████▋                   | 204.2 MB 47.7 MB/s eta 0:00:07     |███████████████                 | 241.2 MB 45.0 MB/s eta 0:00:07     |███████████████████████████▋    | 446.0 MB 38.6 MB/s eta 0:00:02     |█████████████████████████████   | 469.4 MB 43.2 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 35 kB/s s eta 0:00:01                   | 276 kB 33.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.10.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 33.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 33.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow) (49.1.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 60.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 435 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 707 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.14.1)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.4.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 61.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2019.11.28)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 59.5 MB/s eta 0:00:01\n",
      "\u001b[31mERROR: tensorflow-serving-api 1.15.0 has requirement tensorflow~=1.15.0, but you'll have tensorflow 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorboard 2.2.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.10.1 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h5py, astunparse, tensorboard-plugin-wit, cachetools, pyasn1-modules, google-auth, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, gast, tensorflow-estimator, tensorflow\n",
      "\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/home/ec2-user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/ec2-user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/ec2-user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.18.0 google-auth-oauthlib-0.4.1 h5py-2.10.0 oauthlib-3.1.0 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 tensorboard-2.2.2 tensorboard-plugin-wit-1.7.0 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip         # pip 19.0 or higher is required for TF 2\n",
    "!pip install --upgrade setuptools\n",
    "!pip install --user --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
    "!unzip wwm_uncased_L-24_H-1024_A-16.zip\n",
    "\n",
    "!ls wwm_uncased_L-24_H-1024_A-16\n",
    "\n",
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!unzip uncased_L-12_H-768_A-12.zip\n",
    "\n",
    "!ls uncased_L-12_H-768_A-12\n",
    "\n",
    "!pip install simpletransformers\n",
    "!pip install transformers\n",
    "!pip install tensorboardx\n",
    "!pip install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (2.2.0)\n",
      "Requirement already satisfied: erlastic in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bert) (2.0.0)\n",
      "Requirement already satisfied: bert-tensorflow in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bert-tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow_hub in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_hub) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_hub) (1.16.4)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow_hub) (3.8.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow_hub) (49.1.0)\n",
      "Requirement already satisfied: bert-for-tf2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.14.4)\n",
      "Requirement already satisfied: py-params>=0.9.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.47.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.16.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert\n",
    "!pip install bert-tensorflow\n",
    "!pip install tensorflow_hub\n",
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import bert\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "#from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_claims(text_claims):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_seq_len,cnn_filters,dropout_rate,dnn_units):\n",
    "    input_ids = keras.layers.Input(\n",
    "                                   shape=(max_seq_len, ),\n",
    "                                   dtype='int32',\n",
    "                                   name=\"input_ids\"\n",
    "                                   )\n",
    "    input_mask = keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    \n",
    "    _, bert_output = bert_layer([input_ids, input_mask, segment_ids])\n",
    "    #bert_output = bert_layer(input_ids)\n",
    "    print(\"bert shape\", bert_output.shape)\n",
    "    \n",
    "    \n",
    "    cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")(bert_output)\n",
    "    cnn_layer1 = layers.GlobalMaxPool1D()(cnn_layer1)\n",
    "    cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")(bert_output)\n",
    "    cnn_layer2 = layers.GlobalMaxPool1D()(cnn_layer2)\n",
    "    cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")(bert_output)\n",
    "    cnn_layer3 = layers.GlobalMaxPool1D()(cnn_layer3)\n",
    "\n",
    "    concatenated = tf.concat([cnn_layer1, cnn_layer2, cnn_layer3], axis=-1) # (batch_size, 3 * cnn_filters)   \n",
    "    dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")(concatenated)\n",
    "    dropout = layers.Dropout(rate=dropout_rate)(concatenated)\n",
    "       \n",
    "    last_dense = layers.Dense(units=1,activation=\"sigmoid\")(dropout)\n",
    "    \n",
    "    model = keras.Model(inputs=[input_ids,input_mask,segment_ids], outputs=last_dense)\n",
    "    \n",
    "    #cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "    #cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "    #logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "    #logits = keras.layers.Dropout(0.5)(logits)\n",
    "    #logits = keras.layers.Dense(\n",
    "    #units=len(classes),\n",
    "    #activation=\"softmax\"\n",
    "    #)(logits)\n",
    "    #model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "    model.build(input_shape=(None, max_seq_len))\n",
    "    #load_stock_weights(bert, bert_ckpt_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://trainedbertmodelweights/BERT_EMBEDDINGS_TRAINABLE_CNN_weights-improvement-19-0.98.hdf5\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket='trainedbertmodelweights'\n",
    "data_key = 'BERT_EMBEDDINGS_TRAINABLE_CNN_weights-improvement-19-0.98.hdf5'\n",
    "weights_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "print(weights_location)\n",
    "\n",
    "local_weights_file = 'weights.hdf5'\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.download_file(bucket, data_key, local_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of English Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
    "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\",\"she's\":\"she is\",\"he's\":\"he is\"}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    #convert ’ to '\n",
    "    \n",
    "    sentence = sen.replace(\"’\",\"'\")\n",
    "    \n",
    "    #expand contractions \n",
    "    sentence = expand_contractions(sentence.lower())\n",
    "    # Removing punctuation\n",
    "    #sentence = re.sub('<[^>]+>',' ', sen)\n",
    "\n",
    "    # Remove punctuations and numbers and foreign characters\n",
    "    sentence = re.sub('[^a-zA-Z]',' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\",' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+',' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, None, 768)\n"
     ]
    }
   ],
   "source": [
    "precision_obj = tf.keras.metrics.Precision()\n",
    "recall_obj = tf.keras.metrics.Recall()\n",
    "#Get model for Experiment3 created\n",
    "CNN_FILTERS = 200\n",
    "DROPOUT_RATE = 0.1\n",
    "DNN_UNITS = 128\n",
    "model = create_model(60,CNN_FILTERS,DROPOUT_RATE,DNN_UNITS)\n",
    "#load model weights\n",
    "model.load_weights(\"./weights.hdf5\")\n",
    "\n",
    "model.compile(\n",
    "               optimizer=keras.optimizers.Adam(1e-5),\n",
    "               loss='binary_crossentropy',\n",
    "               run_eagerly=True,\n",
    "               metrics=['accuracy',precision_obj,recall_obj]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_claims(claim_text):\n",
    "    claims = [preprocess_text(claim_text)]\n",
    "\n",
    "    max_seq_len=60\n",
    "    x_input,masks,segments = [], [],[]\n",
    "    for new_claim in claims:\n",
    "        text = tokenizer.tokenize(new_claim)\n",
    "        text = text[:max_seq_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_seq_len - len(input_sequence)\n",
    "        print(text)\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_seq_len\n",
    "        \n",
    "        #tokens = tf.cast(tokens, tf.int32)\n",
    "        #pad_masks = tf.cast(pad_masks, tf.int32)\n",
    "        #segment_ids = tf.cast(segment_ids, tf.int32)\n",
    "        \n",
    "        x_input.append(np.array(tokens))\n",
    "        masks.append(np.array(pad_masks))\n",
    "        segments.append(np.array(segment_ids))\n",
    "        print(tokens)\n",
    "        print(pad_masks)\n",
    "        print(segment_ids)\n",
    "\n",
    "    print('return')\n",
    "    return json.dumps({\"instances\": [{\n",
    "            \"input_ids\": tokens,\n",
    "            \"input_mask\": pad_masks,\n",
    "            \"segment_ids\": segment_ids\n",
    "        }]})\n",
    "\n",
    "    #return model.predict([x_input,masks,segments])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks spread the coronavirusd\n",
      "['masks', 'spread', 'the', 'corona', '##virus', '##d']\n",
      "[101, 15806, 3659, 1996, 21887, 23350, 2094, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "return\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"instances\": [{\"input_ids\": [101, 15806, 3659, 1996, 21887, 23350, 2094, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"input_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"segment_ids\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}]}'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_claims('masks spread the coronavirusd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
